{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xq8CzSmSbc4D",
        "E8HId1HZdhsa",
        "EmOLiyZHxiMs"
      ],
      "authorship_tag": "ABX9TyNEqtqtpkyeIionWjOD3Dzq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TK-brsq/Research/blob/main/Han2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Membrane Potential Neuron for Enabling Deep High-accuracy and Low-Latency Spiking Neural Network"
      ],
      "metadata": {
        "id": "XlUSjf73bSua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import and Load Data"
      ],
      "metadata": {
        "id": "xq8CzSmSbc4D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf2OjuEJbQaO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.functional as F\n",
        "import torch.utils as utils\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch"
      ],
      "metadata": {
        "id": "r1dHWt1ocIww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "597f39c8-e331-4a40-87f9-533c4309f246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.4.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.26.4)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-1.0.4-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Downloading snntorch-0.9.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nir-1.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: nir, nirtorch, snntorch\n",
            "Successfully installed nir-1.0.4 nirtorch-1.0 snntorch-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "import snntorch.utils as sutils\n",
        "import snntorch.functional as sF"
      ],
      "metadata": {
        "id": "-YFzr8CGcHKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n",
        "\n",
        "train = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Or-mKNZzcpeY",
        "outputId": "3b0ba474-7fd9-458a-d200-b49230e1bb88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 69974681.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG"
      ],
      "metadata": {
        "id": "E8HId1HZdhsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, 3, 1, 1, bias=False),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(2),\n",
        "    nn.Conv2d(32, 64, 3, 1, 1, bias=False),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(2),\n",
        "    nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(128, 128, 3, 1, 1, bias=False),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(2),\n",
        "    nn.Flatten(),\n",
        "    nn.Dropout(p=0.4),\n",
        "    nn.Linear(2048, 2048, bias=False),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.4),\n",
        "    nn.Linear(2048, 2048, bias=False),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.4),\n",
        "    nn.Linear(2048, 10, bias=False)\n",
        ")\n",
        "#decay=2e-4, mile=[2,4,6,7], epochs=8, [83,612, 78.42]\n",
        "#p = 0.5, decay=2e-4, mile=[2,4,6,7,8,9], epochs=10, [76.814, 77.18]\n",
        "#vgg7, p=0.4, decay=1e-4, mile=[2,4,6,8,9], epochs=10, [81.656, 79.33]\n",
        "#32P-64P-128+128P-2048-2048-10,p=0.4, decay=1e-4, exp(0.8), epochs=10, [82.364, 78.55]"
      ],
      "metadata": {
        "id": "F_v1tXs3epFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation = {}\n",
        "def named_hook(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach().cpu().numpy()\n",
        "    return hook"
      ],
      "metadata": {
        "id": "jw3NGfbDf9Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(cnn.parameters(), weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.8)\n",
        "criteria = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "bM4Aaa54gvXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "device = torch.device('cuda')\n",
        "cnn.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    tr_loss = 0\n",
        "    tr_correct = 0\n",
        "    cnn.train()\n",
        "    for data, target in tqdm(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = cnn(data)\n",
        "        loss = criteria(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        _, pred = out.max(1)\n",
        "        tr_correct += (pred == target).sum().item()\n",
        "    scheduler.step()\n",
        "\n",
        "    #register activation\n",
        "    if epoch == epochs - 1:\n",
        "        for idx, layer in enumerate(cnn):\n",
        "            if isinstance(layer, nn.ReLU):\n",
        "                layer.register_forward_hook(named_hook(f'{idx}ReLU'))\n",
        "        cnn[19].register_forward_hook(named_hook('ReLU19'))\n",
        "        with torch.no_grad():\n",
        "            for data, target in train_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                out = cnn(data)\n",
        "\n",
        "    ts_loss = 0\n",
        "    ts_correct = 0\n",
        "    cnn.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            out = cnn(data)\n",
        "\n",
        "            ts_loss += criteria(out, target).item()\n",
        "            _, pred = out.max(1)\n",
        "            ts_correct += (pred == target).sum().item()\n",
        "\n",
        "    print(f'Epoch : {epoch}')\n",
        "    print(f'train : {tr_loss/50000}, {tr_correct/500}%\\ttest : {ts_loss/10000}, {ts_correct/100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rzBpFy6ghbCg",
        "outputId": "fbf00a05-2098-4fa2-8f12-57d005078dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:22<00:00, 34.42it/s]\n",
            "100%|██████████| 157/157 [00:03<00:00, 44.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "train : 0.025018361573219298, 40.898%\ttest : 0.020043831557035447, 53.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:21<00:00, 37.12it/s]\n",
            "100%|██████████| 157/157 [00:04<00:00, 31.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1\n",
            "train : 0.018751052986383437, 57.668%\ttest : 0.016441240674257278, 63.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:24<00:00, 32.04it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 53.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 2\n",
            "train : 0.015386248117685318, 65.664%\ttest : 0.01471907422542572, 67.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:21<00:00, 35.68it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 56.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 3\n",
            "train : 0.013279960027933121, 70.458%\ttest : 0.012736618757247924, 71.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:21<00:00, 37.18it/s]\n",
            "100%|██████████| 157/157 [00:02<00:00, 53.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 4\n",
            "train : 0.011801668327450752, 73.772%\ttest : 0.012071572357416153, 73.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:21<00:00, 37.06it/s]\n",
            "100%|██████████| 157/157 [00:03<00:00, 49.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 5\n",
            "train : 0.010523487982153893, 76.362%\ttest : 0.010956252211332321, 76.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:20<00:00, 38.49it/s]\n",
            "100%|██████████| 157/157 [00:04<00:00, 36.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 6\n",
            "train : 0.00971093220293522, 78.6%\ttest : 0.010674732801318169, 76.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:21<00:00, 35.69it/s]\n",
            "100%|██████████| 157/157 [00:03<00:00, 49.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 7\n",
            "train : 0.008977935937643051, 79.974%\ttest : 0.010577488261461258, 77.1%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:20<00:00, 38.59it/s]\n",
            "100%|██████████| 157/157 [00:03<00:00, 43.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 8\n",
            "train : 0.008303053728044033, 81.568%\ttest : 0.010182826688885689, 78.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:20<00:00, 38.23it/s]\n",
            "100%|██████████| 157/157 [00:03<00:00, 42.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 9\n",
            "train : 0.007867997389733792, 82.364%\ttest : 0.00987781553864479, 78.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "cnn.to(device)\n",
        "torch.save(cnn.state_dict(), 'hen2020.pth')\n",
        "np.save('activation_hen.npy', activation)"
      ],
      "metadata": {
        "id": "MLy96EEQuYqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  SNN and Conversion"
      ],
      "metadata": {
        "id": "bXI2YkHOtGcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scnn_seq = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, 3, 1, 1, bias=False),\n",
        "    snn.Leaky(1, init_hidden=True),\n",
        "    nn.AvgPool2d(2),\n",
        "    nn.Conv2d(32, 64, 3, 1, 1, bias=False),\n",
        "    snn.Leaky(1, init_hidden=True),\n",
        "    nn.AvgPool2d(2),\n",
        "    nn.Conv2d(64, 128, 3, 1, 1, bias=False),\n",
        "    snn.Leaky(1, init_hidden=True),\n",
        "    nn.Conv2d(128, 128, 3, 1, 1, bias=False),\n",
        "    snn.Leaky(1, init_hidden=True),\n",
        "    nn.AvgPool2d(2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(2048, 2048, bias=False),\n",
        "    snn.Leaky(1, init_hidden=True),\n",
        "    nn.Linear(2048, 2048, bias=False),\n",
        "    snn.Leaky(1, init_hidden=True),\n",
        "    nn.Linear(2048, 10, bias=False),\n",
        "    snn.Leaky(1)\n",
        ")"
      ],
      "metadata": {
        "id": "YHbGmPUctHxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1, 1, bias=False)\n",
        "        self.lif1 = snn.Leaky(1, init_hidden=True)\n",
        "        self.conv2 = nn.Conv2d(32, 32, 3, 1, 1, bias=False)\n",
        "        self.lif2 = snn.Leaky(1, init_hidden=True)\n",
        "        self.pool1 = nn.AvgPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n",
        "        self.lif3 = snn.Leaky(1, init_hidden=True)\n",
        "        self.conv4 = nn.Conv2d(64, 64, 3, 1, 1, bias=False)\n",
        "        self.lif4 = snn.Leaky(1, init_hidden=True)\n",
        "        self.pool2 = nn.AvgPool2d(2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(4096, 1024, bias=False)\n",
        "        self.lif5 = snn.Leaky(1, init_hidden=True)\n",
        "        self.fc2 = nn.Linear(1024, 256, bias=False)\n",
        "        self.lif6 = snn.Leaky(1, init_hidden=True)\n",
        "        self.fc3 = nn.Linear(256, 10, bias=False)\n",
        "        self.lif7 = snn.Leaky(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lif1(self.conv1(x))\n",
        "        x = self.pool1(self.lif2(self.conv2(x)))\n",
        "        x = self.lif3(self.conv3(x))\n",
        "        x = self.pool2(self.lif4(self.conv4(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.lif5(self.fc1(x))\n",
        "        x = self.lif6(self.fc2(x))\n",
        "        spk, mem = self.lif7(self.fc3(x))\n",
        "        return spk, mem\n",
        "\n",
        "scnn_module = SCNN()"
      ],
      "metadata": {
        "id": "-cEW_9uiFmyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seq 2 module\n",
        "cnn_sd = torch.load('hen2020.pth', weights_only=True)\n",
        "\n",
        "scnn_sd = {}\n",
        "scnn_sd['conv1.weight'] = cnn_sd['0.weight']\n",
        "scnn_sd['conv2.weight'] = cnn_sd['2.weight']\n",
        "scnn_sd['conv3.weight'] = cnn_sd['5.weight']\n",
        "scnn_sd['conv4.weight'] = cnn_sd['7.weight']\n",
        "scnn_sd['fc1.weight'] = cnn_sd['12.weight']\n",
        "scnn_sd['fc2.weight'] = cnn_sd['15.weight']\n",
        "scnn_sd['fc3.weight'] = cnn_sd['18.weight']\n",
        "\n",
        "scnn_module.load_state_dict(scnn_sd, strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HquGt3ff51J1",
        "outputId": "9dc6e49d-98be-432d-e0f2-256ebe577d8c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['lif1.threshold', 'lif1.graded_spikes_factor', 'lif1.reset_mechanism_val', 'lif1.beta', 'lif2.threshold', 'lif2.graded_spikes_factor', 'lif2.reset_mechanism_val', 'lif2.beta', 'lif3.threshold', 'lif3.graded_spikes_factor', 'lif3.reset_mechanism_val', 'lif3.beta', 'lif4.threshold', 'lif4.graded_spikes_factor', 'lif4.reset_mechanism_val', 'lif4.beta', 'lif5.threshold', 'lif5.graded_spikes_factor', 'lif5.reset_mechanism_val', 'lif5.beta', 'lif6.threshold', 'lif6.graded_spikes_factor', 'lif6.reset_mechanism_val', 'lif6.beta', 'lif7.threshold', 'lif7.graded_spikes_factor', 'lif7.reset_mechanism_val', 'lif7.beta'], unexpected_keys=[])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seq 2 seq\n",
        "cnn_sd = torch.load('hen2020.pth', weights_only=True)\n",
        "\n",
        "scnn_sd = {}\n",
        "scnn_sd = {k: v for k, v in cnn_sd.items() if k in scnn_seq.state_dict()}\n",
        "scnn_sd['12.weight'] = cnn_sd['13.weight']\n",
        "scnn_sd['14.weight'] = cnn_sd['16.weight']\n",
        "scnn_sd['16.weight'] = cnn_sd['19.weight']\n",
        "\n",
        "scnn_seq.load_state_dict(scnn_sd, strict=False)"
      ],
      "metadata": {
        "id": "rUP7zjDdwXFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40db171-9d91-4f1b-ce6d-9a03d8d10096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['1.threshold', '1.graded_spikes_factor', '1.reset_mechanism_val', '1.beta', '4.threshold', '4.graded_spikes_factor', '4.reset_mechanism_val', '4.beta', '7.threshold', '7.graded_spikes_factor', '7.reset_mechanism_val', '7.beta', '9.threshold', '9.graded_spikes_factor', '9.reset_mechanism_val', '9.beta', '13.threshold', '13.graded_spikes_factor', '13.reset_mechanism_val', '13.beta', '15.threshold', '15.graded_spikes_factor', '15.reset_mechanism_val', '15.beta', '17.threshold', '17.graded_spikes_factor', '17.reset_mechanism_val', '17.beta'], unexpected_keys=[])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(scnn_seq.state_dict(), 'hen2020_snn.pth')"
      ],
      "metadata": {
        "id": "KwUwB1pVaFXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Threshold Balancing By ReLU"
      ],
      "metadata": {
        "id": "EmOLiyZHxiMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation_obj = np.load('activation_hen.npy', allow_pickle=True)\n",
        "activation = activation_obj.item()\n",
        "\n",
        "thresholds = []\n",
        "for k, v in activation.items():\n",
        "    m = np.max(v)\n",
        "    p = np.percentile(v, q=99.9)\n",
        "    thresholds.append(p)\n",
        "    print(k, m, p)"
      ],
      "metadata": {
        "id": "rm-YQsDaPtzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959e562c-9806-40d6-c61d-b5b28421fc33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1ReLU 2.015399 1.3645718150138977\n",
            "3ReLU 3.5909607 1.7925383200645553\n",
            "6ReLU 2.3062818 0.9410146411657363\n",
            "8ReLU 1.7592487 0.9739432749748534\n",
            "13ReLU 3.0144727 2.0458919336795867\n",
            "16ReLU 6.80631 5.817738733291929\n",
            "linear 11.839191 11.79015914821625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Threshold balancing by ReLU\n",
        "#seq to module\n",
        "scnn_module.lif1.threshold = torch.tensor(thresholds[0])\n",
        "scnn_module.lif2.threshold = torch.tensor(thresholds[1])\n",
        "scnn_module.lif3.threshold = torch.tensor(thresholds[2])\n",
        "scnn_module.lif4.threshold = torch.tensor(thresholds[3])\n",
        "#scnn_module.lif5.threshold = torch.tensor(thresholds[4])\n",
        "#scnn_module.lif6.threshold = torch.tensor(thresholds[5])\n",
        "#scnn_module.lif7.threshold = torch.tensor(thresholds[6])"
      ],
      "metadata": {
        "id": "_24y-ljw9Nqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Threshold balancing by ReLU\n",
        "#seq to seq\n",
        "for idx, layer in enumerate(scnn_module):\n",
        "    if isinstance(layer, snn.Leaky):\n",
        "        layer.threshold = torch.tensor(thresholds[idx])\n",
        "    #if idx == 6: break"
      ],
      "metadata": {
        "id": "3DE6GDxz9KZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Threshold Balancing by IF"
      ],
      "metadata": {
        "id": "ToIR4-Sa3KCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scnn_sd = torch.load('hen2020_snn.pth', weights_only=True)\n",
        "scnn_seq.load_state_dict(scnn_sd, strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y66aqooCalaQ",
        "outputId": "14e059d4-455a-4bdc-ab5d-cfcd1080c912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TB_loader = DataLoader(train, batch_size=256, shuffle=True)\n",
        "data, target = next(iter(TB_loader))"
      ],
      "metadata": {
        "id": "mGOk7YqpcRJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 8\n",
        "with torch.no_grad():\n",
        "    sutils.reset(scnn_seq)\n",
        "    #first Leaky\n",
        "    out_list = []\n",
        "    for step in range(steps):\n",
        "        out = scnn_seq[0](data)\n",
        "        out_list.append(out)\n",
        "    out_stack = torch.stack(out_list)\n",
        "    p999 = np.percentile(out_stack.numpy(), q=99.9)\n",
        "    scnn_seq[1].threshold = torch.tensor(p999)\n",
        "    #second Leaky\n",
        "    out_list[]\n",
        "    for step in out_stack:\n",
        "        out = scnn_seq[1, 2, 3](step)\n",
        "        out_list.append(out)\n",
        "    out_stack = torch.stack(out_list)\n",
        "    p999 = np.percentile(out_stack.numpy(), q=99.9)\n",
        "    scnn_seq[4].threshold = torch.tensor(p999)\n",
        "    #third Leaky\n",
        "    out_list[]\n",
        "    for step in out_stack:\n",
        "        out = scnn_seq[4, 5, 6](step)\n",
        "        out_list.append(out)\n",
        "    out_stack = torch.stack(out_list)\n",
        "    p999 = np.percentile(out_stack.numpy(), q=99.9)\n",
        "    scnn_seq[7].threshold = torch.tensor(p999)\n",
        "    #forth Leaky\n",
        "    out_list[]\n",
        "    for step in out_stack:\n",
        "        out = scnn_seq[7, 8](step)\n",
        "        out_list.append(out)\n",
        "    out_stack = torch.stack(out_list)\n",
        "    p999 = np.percentile(out_stack.numpy(), q=99.9)\n",
        "    scnn_seq[9].threshold = torch.tensor(p999)\n",
        "    #fifth Leaky\n",
        "    out_list[]\n",
        "    for step in out_stack:\n",
        "        out = scnn_seq[9, 10, 11, 12](step)\n",
        "        out_list.append(out)\n",
        "    out_stack = torch.stack(out_list)\n",
        "    p999 = np.percentile(out_stack.numpy(), q=99.9)\n",
        "    scnn_seq[13].threshold = torch.tensor(p999)\n",
        "    #sixth Leaky\n",
        "    out_list[]\n",
        "    for step in out_stack:\n",
        "        out = scnn_seq[13, 14](step)\n",
        "        out_list.append(out)\n",
        "    out_stack = torch.stack(out_list)\n",
        "    p999 = np.percentile(out_stack.numpy(), q=99.9)\n",
        "    scnn_seq[15].threshold = torch.tensor(p999)\n",
        "    #seventh\n",
        "    out_list[]\n",
        "    for step in out_stack:\n",
        "        out = scnn_seq[15, 16](step)\n",
        "        out_list.append(out)\n",
        "    out_stack = torch.stack(out_list)\n",
        "    p999 = np.percentile(out_stack.numpy(), q=99.9)\n",
        "    scnn_seq[17].threshold = torch.tensor(p999)\n",
        "    #out\n",
        "    out_list = []\n",
        "    for step in out_stack:\n",
        "        out = scnn_seq[17](step)\n",
        "        out_list.append(out)\n",
        "    out_stack = torch.stack(out_list)\n",
        "    print(out_stack.shape)"
      ],
      "metadata": {
        "id": "HUpT99K4ZRsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result"
      ],
      "metadata": {
        "id": "-AzJ-F7Dxom_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criteria = sF.ce_count_loss()"
      ],
      "metadata": {
        "id": "VDvY3BUlwtz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "timesteps = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "scnn_module.to(device)\n",
        "scnn_module.eval()\n",
        "with torch.no_grad():\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    for data, target in tqdm(test_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        sutils.reset(scnn_module)\n",
        "\n",
        "        spk_step = []\n",
        "        for step in range(timesteps):\n",
        "            spk, mem = scnn_module(data)\n",
        "            spk_step.append(spk)\n",
        "        spk_batch = torch.stack(spk_step)\n",
        "\n",
        "        loss += criteria(spk_batch, target).item()\n",
        "        _, pred = spk_batch.sum(0).max(1)\n",
        "        correct += (pred == target).sum().item()\n",
        "\n",
        "        del spk_batch, spk_step, data, target\n",
        "\n",
        "    print(f'\\n{loss/10000},\\t{correct/100}%')\n",
        "\n",
        "#by ReLU\n",
        "#99.9, steps=32, [, 72.02], linear後のreluのbalancingなし\n",
        "#99.9, steps=32, [, 10]\n",
        "\n",
        "#by IFz\n",
        "#max, steps=32, for文回さずbalancing, [, 15.17]"
      ],
      "metadata": {
        "id": "mAcG_TpPxq83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426a43de-9ccd-433b-b712-aef82a3e4cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:23<00:00,  6.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0.1995559422492981,\t15.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
