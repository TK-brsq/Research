{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mHA80rTjPKzH"
      ],
      "authorship_tag": "ABX9TyP07MS3bUZBsxaEddCHsSPr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TK-brsq/Research/blob/main/SimSiam_by_SEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzgLck_XNHmI"
      },
      "outputs": [],
      "source": [
        "#!pip install spikingjelly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler as lrs\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision.transforms import v2 as TF\n",
        "from torchvision import datasets\n",
        "'''\n",
        "import spikingjelly\n",
        "from spikingjelly import layer as jnn, neuron, functional as jF\n",
        "'''\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ],
      "metadata": {
        "id": "inE61C2fOS4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "mHA80rTjPKzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplane, outplane, down_sampling=True):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.down_sampling = down_sampling\n",
        "        self.stride = 2 if down_sampling else 1\n",
        "        self.down_sample  = jnn.Sequential(\n",
        "            jnn.Conv2d(inplane, outplane, 3, 2, 1, bias=False),\n",
        "            jnn.BatchNorm2d(outplane),\n",
        "            neuron.IFNode()\n",
        "        )\n",
        "\n",
        "        layer = []\n",
        "        layer.append(jnn.Conv2d(inplane, outplane, 3, self.stride, 1, bias=False))\n",
        "        layer.append(jnn.BatchNorm2d(outplane))\n",
        "        layer.append(jnn.IFNode())\n",
        "        layer.append(jnn.Conv2d(outplane, outplane, 3, 1, 1, bias=False))\n",
        "        layer.append(jnn.BatchNorm2d(outplane))\n",
        "        layer.append(neuron.IFNode())\n",
        "        self.layer = jnn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.layer(x)\n",
        "        if self.down_sampling:\n",
        "            identity = self.conv1x1(identity)\n",
        "        x += identity\n",
        "        return x\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        self.first = nn.Sequential(\n",
        "            jnn.Conv2d(3, 32, 3, 1, 1, bias=False),\n",
        "            jnn.BatchNorm2d(32)\n",
        "        )\n",
        "        self.block1 = BasicBlock(32, 32, False)\n",
        "        self.block2 = BasicBlock(32, 32, False)\n",
        "        self.block3 = BasicBlock(32, 64, True)\n",
        "        self.block4 = BasicBlock(64, 64, False)\n",
        "        self.block5 = BasicBlock(64, 128, True)\n",
        "        self.block6 = BasicBlock(128, 128, False)\n",
        "        self.block7 = BasicBlock(128, 256, True)\n",
        "        self.block8 = BasicBlock(256, 256, False)\n",
        "        self.projector = nn.Sequential(\n",
        "            jnn.AdaptiveAvgPool2d((1, 1)),\n",
        "            jnn.Linear(256, 1024, bias=False),\n",
        "            jnn.BatchNorm1d(1024),\n",
        "            neuron.IFNode(),\n",
        "            jnn.Linear(1024, 1024, bias=False)\n",
        "        )\n",
        "\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, jnn.Conv2d):\n",
        "                jnn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
        "            elif isinstance(layer, nn.BatchNorm2d):\n",
        "                nn.init.ones_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first(x)\n",
        "        x = self.block1(x) #[64, 32, 32] -> [64, 32, 32]\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x) #[64, 32, 32] -> [128, 16, 16]\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x) #[128, 16, 16] -> [256, 8, 8]\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x) #[256, 8, 8] -> [512, 4, 4]\n",
        "        x = self.block8(x)\n",
        "        x = self.last(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_8s8xpOlPN9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimSiam(nn.Module):\n",
        "    def __init__(self, encoder, indim=512):\n",
        "        super(SimSiam, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.projector = nn.Sequential(\n",
        "            jnn.Linear(indim, indim//4, bias=False),\n",
        "            jnn.ReLU(),\n",
        "            jnn.Linear(indim//4, indim, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, xi, xj):\n",
        "        hi = self.encoder(xi)\n",
        "        hj = self.encoder(xj)\n",
        "        zi = self.projector(hi)\n",
        "        zj = self.projector(hj)\n",
        "        return zi, zj"
      ],
      "metadata": {
        "id": "ELiZmzyxZhjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, indim, classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            jnn.Linear(indim, classes, bias=False),\n",
        "            neuron.IFNode()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        y = self.layer(x)\n",
        "        return y"
      ],
      "metadata": {
        "id": "kGhgK9xRkwxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NT_Xent(nn.Module):\n",
        "    def __init__(self, batch_size, tau, device):\n",
        "        super(NT_Xent, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.device = device\n",
        "        self.mask = self.make_mask()\n",
        "        self.cosine = nn.CosineSimilarity(dim=2)\n",
        "        self.Xent = nn.CrossEntropyLoss()\n",
        "\n",
        "    def make_mask(self):\n",
        "        mask = torch.eye(2 * self.batch_size)\n",
        "        ones = torch.ones(self.batch_size)\n",
        "        mask = mask + torch.diag(ones, self.batch_size) + torch.diag(ones, -self.batch_size)\n",
        "        return ~mask.bool()\n",
        "\n",
        "    def forward(self, zi, zj):\n",
        "        z = torch.cat((zi, zj), dim=0)\n",
        "        similarity = self.cosine(z.unsqueeze(1), z.unsqueeze(0)) / self.tau\n",
        "\n",
        "        sim_ij, sim_ji = torch.diag(similarity, self.batch_size), torch.diag(similarity, -self.batch_size)\n",
        "        positive = torch.cat((sim_ij, sim_ji), dim=0).reshape(2*self.batch_size, 1)\n",
        "        negative = similarity[self.mask].reshape(2*self.batch_size, -1)\n",
        "\n",
        "        labels = torch.zeros(2*self.batch_size, dtype=torch.long).to(self.device)\n",
        "        logits = torch.cat((positive, negative), dim=1)\n",
        "        loss = self.Xent(logits, labels)\n",
        "\n",
        "        return loss / 2"
      ],
      "metadata": {
        "id": "tjhg4XyRPaaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "9ncaaIgoVmnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentation:\n",
        "    def __init__(self):\n",
        "        self.device = device\n",
        "        color_jitter = TF.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "        self.tf = TF.Compose([\n",
        "            TF.RandomResizedCrop(32, (0.36, 1)),\n",
        "            TF.RandomHorizontalFlip(p=0.5),\n",
        "            TF.RandomApply([color_jitter], p=0.8),\n",
        "            TF.RandomGrayscale(p=0.2),\n",
        "            TF.ToImage(),\n",
        "            TF.ToDtype(torch.float32, scale=True)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.tf(x), self.tf(x)"
      ],
      "metadata": {
        "id": "c1QjfoDk1DX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(data='cifar10', split='train', batch_size=128, DA=False):\n",
        "    tf = DataAugmentation() if DA else TF.Compose([TF.ToImage(), TF.ToDtype(torch.float32, scale=True)])\n",
        "    if data == 'cifar10':\n",
        "        match split:\n",
        "            case 'train':\n",
        "                data = datasets.CIFAR10('./data', train=True, tranform=tf, download=True)\n",
        "            case 'test':\n",
        "                data = datasets.CIFAR10('./data', train=False, tranform=tf, download=True)\n",
        "            case 'all':\n",
        "                train = datasets.CIFAR10('./data', train=True, tranform=tf, download=True)\n",
        "                test = datasets.CIFAR10('./data', train=False, tranform=tf, download=True)\n",
        "                data = ConcatDataset([train, test])\n",
        "    elif data == 'stl10':\n",
        "        match split:\n",
        "            case 'train':\n",
        "                data = datasets.STL10('./data', split='train', tranform=tf, download=True)\n",
        "            case 'test':\n",
        "                data = datasets.STL10('./data', split='test', tranform=tf, download=True)\n",
        "            case 'all':\n",
        "                data = datasets.STL10('./data', split='unlabeled', tranform=tf, download=True)\n",
        "    else:\n",
        "        print(f'{data} is not supported >_<. cifar10 or stl10 is supported')\n",
        "    loader = DataLoader(data, batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "uMIXsB32yVvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_(loader, model, optimizer, scheduler, criterion, device):\n",
        "    jF.reset_net(model)\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    model.train()\n",
        "    for data, target in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        out = model(data)\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #xm.optimizer_step(optimizer)\n",
        "        running_loss += loss.item()\n",
        "        correct += (out.argmax(1) == target).sum().item()\n",
        "    scheduler.step()\n",
        "    return running_loss, correct"
      ],
      "metadata": {
        "id": "QCe3SHrhaGUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cl(loader, model, optimizer, scheduler, criterion, device):\n",
        "    jF.reset_net(model)\n",
        "    running_loss = 0\n",
        "    model.train()\n",
        "    for (x1, x2), _ in tqdm(loader):\n",
        "        x1, x2 = x1.to(device), x2.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        z1, z2 = model(x1, x2)\n",
        "        #zi, zj = zi.to(device), zj.to(device)\n",
        "        loss = criterion(z1, z2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #xm.optimizer_step(optimizer)\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    return running_loss"
      ],
      "metadata": {
        "id": "g4lcasnwVpqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_in_cl(loader, encoder, classifier, optimizer, criterion, device):\n",
        "    jF.reset_net(encoder)\n",
        "    jF.reset_net(classifier)\n",
        "    correct = 0\n",
        "    encoder.eval()\n",
        "    classifier.train()\n",
        "    for data, target in loader:\n",
        "        optimizer.zero_grad()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        with torch.no_grad():\n",
        "            z = encoder(data)\n",
        "        out = classifier(z)\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #xm.optimizer_step(optimizer)\n",
        "        correct += (out.argmax(1) == target).sum().item()\n",
        "    return correct"
      ],
      "metadata": {
        "id": "9t9Ow04Q4kZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(filename, model, optimizer, scheduler):\n",
        "    checkpoint = {\n",
        "        'model_sd': model.state_dict(),\n",
        "        'optimizer_sd': optimizer.state_dict(),\n",
        "        'scheduler_sd': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(checkpoint, f'{filename}.pth')\n",
        "\n",
        "def load_checkpoint(filename, model, optimizer, scheduler):\n",
        "    checkpoint = torch.load(f'{filename}.pth')\n",
        "    model.load_state_dict(checkpoint['model_sd'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_sd'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_sd'])"
      ],
      "metadata": {
        "id": "AQ-911kihgC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "sv6z2_DA_3Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instance\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch_xla.device()\n",
        "#------------------------#\n",
        "loader = get_loader('cifar10', split='train', batch_size=512, DA=True)\n",
        "loader2 = get_loader('cifar10', split='test', batch_size=128, DA=False)\n",
        "N = len(loader2.dataset)\n",
        "#------------------------#\n",
        "encoder = ResNet18().to(device)\n",
        "#model = SimCLR(encoder, 512).to(device)\n",
        "#classifier = Classifier(512, 10).to(device)\n",
        "#------------------------#\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "#optimizer2 = optim.Adam(classifier.parameters())\n",
        "#scheduler1 = lrs.LinearLR(optimizer, start_factor=0.01, total_iters=8)\n",
        "scheduler2 = lrs.CyclicLR(optimizer, base_lr=1e-4, max_lr=5e-4, step_size_up=5)\n",
        "#scheduler = lrs.SequentialLR(optimizer, [scheduler1, scheduler2], milestones=[8])\n",
        "#------------------------#\n",
        "criterion = NT_Xent(512, 0.2, device)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "'''\n",
        "wandb.login()\n",
        "run = wandb.init(\n",
        "    project = 'name',\n",
        "    config = {\n",
        "        'Architecture': 'x',\n",
        "        'optim': 'Adam(1e-3)',\n",
        "        'sche1': 'x',\n",
        "        'sche2': 'x',\n",
        "        'sche': 'x',\n",
        "        'criterion': 'x',\n",
        "        'Data': 'Cifar10',\n",
        "        'else': 'x'\n",
        "    }\n",
        ")\n",
        "'''\n",
        "#train\n",
        "start_epoch = 0\n",
        "epochs = 8\n",
        "jF.set_step_mode(model, 'm')\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    loss = train_cl(loader, model, optimizer, scheduler, criterion, device)\n",
        "    correct = train_in_cl(loader2, model.encoder, classifier, optimizer2, criterion2, device)\n",
        "    #wandb.log({'loss': loss, 'acc': correct*100/N})\n",
        "    print(f'Epoch: {epoch} | loss: {loss} | acc: {correct*100/N}%')\n",
        "\n",
        "#wandb.finish()\n",
        "#save_checkpoint('!!!', simclr, optimizer, scheduler)"
      ],
      "metadata": {
        "id": "v-dkMx6Y_7Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result\n",
        "#"
      ],
      "metadata": {
        "id": "b-d8ffTYDcoQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}