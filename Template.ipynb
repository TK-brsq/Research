{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnP8pJSC2et/nq1d7CZA9x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TK-brsq/Research/blob/main/Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzgLck_XNHmI"
      },
      "outputs": [],
      "source": [
        "#!pip install spikingjelly\n",
        "#!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler as lrs\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import transforms, datasets\n",
        "'''\n",
        "import spikingjelly\n",
        "from spikingjelly import layer as jnn\n",
        "from spikingjelly import neuron\n",
        "from spikingjelly import functional as jF\n",
        "'''\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ],
      "metadata": {
        "id": "inE61C2fOS4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "mHA80rTjPKzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplane, outplane, down_sampling=True):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.down_sampling = down_sampling\n",
        "        self.stride = 2 if down_sampling else 1\n",
        "        self.conv1x1 = nn.Conv2d(inplane, outplane, 1, 2, bias=False)\n",
        "\n",
        "        layer = []\n",
        "        layer.append(nn.BatchNorm2d(inplane))\n",
        "        layer.append(nn.ReLU())\n",
        "        layer.append(nn.Conv2d(inplane, outplane, 3, self.stride, 1, bias=False))\n",
        "        layer.append(nn.BatchNorm2d(outplane))\n",
        "        layer.append(nn.ReLU())\n",
        "        layer.append(nn.Conv2d(outplane, outplane, 3, 1, 1, bias=False))\n",
        "        self.layer = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.layer(x)\n",
        "        if self.down_sampling:\n",
        "            identity = self.conv1x1(identity)\n",
        "        x += identity\n",
        "        return x\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        self.first = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1, bias=False),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.block1 = BasicBlock(64, 64, False)\n",
        "        self.block2 = BasicBlock(64, 64, False)\n",
        "        self.block3 = BasicBlock(64, 128, True)\n",
        "        self.block4 = BasicBlock(128, 128, False)\n",
        "        self.block5 = BasicBlock(128, 256, True)\n",
        "        self.block6 = BasicBlock(256, 256, False)\n",
        "        self.block7 = BasicBlock(256, 512, True)\n",
        "        self.block8 = BasicBlock(512, 512, False)\n",
        "        self.last = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(1*1*512, classes, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.first(x)\n",
        "        x = self.block1(x) #[64, 32, 32] -> [64, 32, 32]\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x) #[64, 32, 32] -> [128, 16, 16]\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x) #[128, 16, 16] -> [256, 8, 8]\n",
        "        x = self.block6(x)\n",
        "        x = self.block7(x) #[256, 8, 8] -> [512, 4, 4]\n",
        "        x = self.block8(x)\n",
        "        x = self.last(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_8s8xpOlPN9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 64)\n",
        "        )\n",
        "\n",
        "    def forward(self, xi, xj):\n",
        "        hi = self.encoder(xi)\n",
        "        hj = self.encoder(xj)\n",
        "        zi = self.projector(hi)\n",
        "        zj = self.projector(hj)\n",
        "        return zi, zj"
      ],
      "metadata": {
        "id": "ELiZmzyxZhjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 10, bias=False)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        y = self.layer(x)\n",
        "        return y"
      ],
      "metadata": {
        "id": "kGhgK9xRkwxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NT_Xent(nn.Module):\n",
        "    def __init__(self, batch_size, tau, device):\n",
        "        super(NT_Xent, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.device = device\n",
        "        self.mask = self.make_mask()\n",
        "        self.cosine = nn.CosineSimilarity(dim=2)\n",
        "        self.Xent = nn.CrossEntropyLoss()\n",
        "\n",
        "    def make_mask(self):\n",
        "        mask = torch.eye(2 * self.batch_size)\n",
        "        ones = torch.ones(self.batch_size)\n",
        "        mask = mask + torch.diag(ones, self.batch_size) + torch.diag(ones, -self.batch_size)\n",
        "        return ~mask.bool()\n",
        "\n",
        "    def forward(self, zi, zj):\n",
        "        z = torch.cat((zi, zj), dim=0)\n",
        "        similarity = self.cosine(z.unsqueeze(1), z.unsqueeze(0)) / self.tau\n",
        "\n",
        "        sim_ij, sim_ji = torch.diag(similarity, self.batch_size), torch.diag(similarity, -self.batch_size)\n",
        "        positive = torch.cat((sim_ij, sim_ji), dim=0).reshape(2*self.batch_size, 1)\n",
        "        negative = similarity[self.mask].reshape(2*self.batch_size, -1)\n",
        "\n",
        "        labels = torch.zeros(2*self.batch_size, dtype=torch.long).to(self.device)\n",
        "        logits = torch.cat((positive, negative), dim=1)\n",
        "        loss = self.Xent(logits, labels)\n",
        "\n",
        "        return loss / 2"
      ],
      "metadata": {
        "id": "tjhg4XyRPaaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "9ncaaIgoVmnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentation:\n",
        "    def __init__(self):\n",
        "        color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "        self.tf = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(32, (0.49, 1)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomApply([color_jitter], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.tf(x), self.tf(x)"
      ],
      "metadata": {
        "id": "c1QjfoDk1DX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(data='cifar10', split='train', DA=False, batch_size=128):\n",
        "    tf = DataAugmentation() if DA else transforms.ToTensor()\n",
        "    if data == 'cifar10':\n",
        "        match split:\n",
        "            case 'train':\n",
        "                data = datasets.CIFAR10('./data', train=True, transform=tf, download=True)\n",
        "            case 'test':\n",
        "                data = datasets.CIFAR10('./data', train=False, transform=tf, download=True)\n",
        "            case 'all':\n",
        "                train = datasets.CIFAR10('./data', train=True, transform=tf, download=True)\n",
        "                test = datasets.CIFAR10('./data', train=False, transform=tf, download=True)\n",
        "                data = ConcatDataset([train, test])\n",
        "        loader = DataLoader(data, batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "        return loader\n",
        "    else:\n",
        "        print(f'{data} is not supported >_<')"
      ],
      "metadata": {
        "id": "uMIXsB32yVvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_(loader, model, optimizer, scheduler, criterion, device, SNN=False):\n",
        "    #jF.reset_net(model) if SNN\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    model.train()\n",
        "    for data, target in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        out = model(data)\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #xm.optimizer_step(optimizer)\n",
        "        running_loss += loss.item()\n",
        "        correct += (out.argmax(1) == target).sum().item()\n",
        "    scheduler.step()\n",
        "    return running_loss, correct"
      ],
      "metadata": {
        "id": "QCe3SHrhaGUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cl(loader, model, optimizer, scheduler, criterion, device, SNN=False):\n",
        "    #jF.reset_net(model) if SNN\n",
        "    running_loss = 0\n",
        "    model.train()\n",
        "    for (xi, xj), _ in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "        xi, xj = xi.to(device), xj.to(device)\n",
        "        zi, zj = model(xi, xj)\n",
        "        zi, zj = zi.to(device), zj.to(device)\n",
        "        loss = criterion(zi, zj)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #xm.optimizer_step(optimizer)\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    return running_loss"
      ],
      "metadata": {
        "id": "g4lcasnwVpqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_in_cl(loader, encoder, classifier, optimizer, criterion, device, SNN=False):\n",
        "    #jF.reset_net(encoder) if SNN\n",
        "    #jF.reset_net(classifier) if SNN\n",
        "    correct = 0\n",
        "    encoder.eval()\n",
        "    classifier.train()\n",
        "    for data, target in loader:\n",
        "        optimizer.zero_grad()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        with torch.no_grad():\n",
        "            z = encoder(data)\n",
        "        out = classifier(z)\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #xm.optimizer_step(optimizer)\n",
        "        correct += (out.argmax(1) == target).sum().item()\n",
        "    return correct"
      ],
      "metadata": {
        "id": "9t9Ow04Q4kZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(filename, model, optimizer, scheduler):\n",
        "    checkpoint = {\n",
        "        'model_sd': model.state_dict(),\n",
        "        'optimizer_sd': optimizer.state_dict(),\n",
        "        'scheduler_sd': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(checkpoint, f'{filename}.pth')\n",
        "\n",
        "def load_checkpoint(filename, model, optimizer, scheduler):\n",
        "    checkpoint = torch.load(f'{filename}.pth')\n",
        "    model.load_state_dict(checkpoint['model_sd'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_sd'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_sd'])"
      ],
      "metadata": {
        "id": "AQ-911kihgC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "sv6z2_DA_3Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb.login()\n",
        "\n",
        "#instance\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch_xla.device()\n",
        "loader = get_loader('cifar10', split='train', DA=False, batch_size=128)\n",
        "N = len(loader.dataset)\n",
        "resnet = ResNet18().to(device)\n",
        "#simclr = SimCLR(encoder)\n",
        "optimizer = optim.Adam(resnet.parameters())\n",
        "#optimizer2 = optim.Adam(classifier.parameters())\n",
        "scheduler1 = lrs.LinearLR(optimizer, start_factor=0.01, total_iters=8)\n",
        "scheduler2 = lrs.CyclicLR(optimizer, base_lr=1e-4, max_lr=5e-4, step_size_up=5)\n",
        "scheduler = lrs.SequentialLR(optimizer, [scheduler1, scheduler2], milestones=[8])\n",
        "criterion = NT_Xent(512, 0.2, device)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "'''\n",
        "#wandb\n",
        "run = wandb.init(\n",
        "    project = 'name',\n",
        "    config = {\n",
        "        'Architecture': 'x',\n",
        "        'optim': 'Adam(1e-3)',\n",
        "        'sche1': 'x',\n",
        "        'sche2': 'x',\n",
        "        'sche': 'x',\n",
        "        'criterion': 'x',\n",
        "        'Data': 'Cifar10',\n",
        "        'else': 'x'\n",
        "    }\n",
        ")\n",
        "'''\n",
        "#train\n",
        "start_epoch = 0\n",
        "epochs = 8\n",
        "#jF.set_step_mode(model, 'm')\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    loss, correct = train_(loader, resnet, optimizer, scheduler, criterion2, device)\n",
        "    #wandb.log({'loss': loss, 'acc': acc})\n",
        "    print(f'Epoch: {epoch} | loss: {loss} | acc: {correct/N}%')\n",
        "\n",
        "#wandb.finish()\n",
        "#save_checkpoint('name', simclr, optimizer, scheduler)"
      ],
      "metadata": {
        "id": "v-dkMx6Y_7Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result\n",
        "#"
      ],
      "metadata": {
        "id": "b-d8ffTYDcoQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}