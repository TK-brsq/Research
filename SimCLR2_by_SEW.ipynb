{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mHA80rTjPKzH",
        "9ncaaIgoVmnw",
        "imIg0hJyzo8f"
      ],
      "authorship_tag": "ABX9TyMpe91nLa2cz//Xnxam88Oz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TK-brsq/Research/blob/main/SimCLR2_by_SEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzgLck_XNHmI",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158af2e9-eb08-45ec-c2a8-97fbf4acc7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spikingjelly\n",
            "  Downloading spikingjelly-0.0.0.0.14-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (2.5.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (4.66.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (0.20.0+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->spikingjelly) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->spikingjelly) (3.0.2)\n",
            "Downloading spikingjelly-0.0.0.0.14-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spikingjelly\n",
            "Successfully installed spikingjelly-0.0.0.0.14\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall -y tensorflow\n",
        "#!pip install tensorflow-cpu\n",
        "#!pip install tensorflow\n",
        "!pip install spikingjelly\n",
        "!pip install wandb\n",
        "#!pip install torch_xla"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler as lrs\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision.transforms import v2 as TF\n",
        "from torchvision import datasets\n",
        "\n",
        "import spikingjelly\n",
        "from spikingjelly.activation_based import layer as jnn, neuron, functional as jF\n",
        "'''\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "from torch_xla.amp import autocast\n",
        "import torch_xla.debug.metrics as met\n",
        "'''\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ],
      "metadata": {
        "id": "inE61C2fOS4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "mHA80rTjPKzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplane, outplane, down_sampling=True):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.down_sampling = down_sampling\n",
        "        self.stride = 2 if down_sampling else 1\n",
        "        self.down_sample  = nn.Sequential(\n",
        "            jnn.Conv2d(inplane, outplane, 2, 2, bias=False),\n",
        "            jnn.BatchNorm2d(outplane),\n",
        "            neuron.IFNode()\n",
        "        )\n",
        "\n",
        "        layer = []\n",
        "        layer.append(jnn.Conv2d(inplane, outplane, 3, self.stride, 1, 1, 2, bias=False))\n",
        "        layer.append(jnn.BatchNorm2d(outplane))\n",
        "        layer.append(neuron.IFNode())\n",
        "        layer.append(jnn.Conv2d(outplane, outplane, 3, 1, 1, 1, 2, bias=False))\n",
        "        layer.append(jnn.BatchNorm2d(outplane))\n",
        "        layer.append(neuron.IFNode())\n",
        "        self.layer = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.layer(x)\n",
        "        if self.down_sampling:\n",
        "            identity = self.down_sample(identity)\n",
        "        x += identity\n",
        "        return x\n",
        "\n",
        "class SEW_ResNet(nn.Module):\n",
        "    def __init__(self, T=4):\n",
        "        super(SEW_ResNet, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "        self.first = nn.Sequential(\n",
        "            jnn.Conv2d(3, 32, 3, 1, 1, bias=False),\n",
        "            jnn.BatchNorm2d(32),\n",
        "            neuron.IFNode()\n",
        "        )\n",
        "        self.block1 = BasicBlock(32, 32, False)\n",
        "        self.block2 = BasicBlock(32, 32, False)\n",
        "        self.block3 = BasicBlock(32, 64, True)\n",
        "        self.block4 = BasicBlock(64, 64, False)\n",
        "        self.block5 = BasicBlock(64, 128, True)\n",
        "        self.block6 = BasicBlock(128, 128, False)\n",
        "        self.last = nn.Sequential(\n",
        "            jnn.AdaptiveAvgPool2d((1, 1)),\n",
        "            jnn.Flatten()\n",
        "        )\n",
        "\n",
        "        jF.set_step_mode(self, 'm')\n",
        "\n",
        "    def forward(self, x):\n",
        "        jF.reset_net(self)\n",
        "        x = x.unsqueeze(0).repeat(self.T, 1, 1, 1, 1)\n",
        "        x = self.first(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = self.block6(x)\n",
        "        x = self.last(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_8s8xpOlPN9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Projector(nn.Module):\n",
        "    def __init__(self, indim=128, outdim=64):\n",
        "        super(Projector, self).__init__()\n",
        "        self.projector = nn.Sequential(\n",
        "            jnn.Linear(indim, indim, bias=False),\n",
        "            neuron.IFNode(),\n",
        "            jnn.Linear(indim, outdim, bias=False)\n",
        "        )\n",
        "        '''\n",
        "        layer0 = self.projector[0]\n",
        "        nn.init.normal_(layer0.weight, 1/128, 1/128**0.5)\n",
        "        '''\n",
        "        jF.set_step_mode(self, 'm')\n",
        "\n",
        "    def forward(self, h):\n",
        "        jF.reset_net(self)\n",
        "        z = self.projector(h)\n",
        "        return z"
      ],
      "metadata": {
        "id": "qtm3rP_W5Kdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, encoder, projector):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.projector = projector\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        h1, h2 = self.encoder(x1), self.encoder(x2)\n",
        "        z1, z2 = self.projector(h1), self.projector(h2)\n",
        "        return z1.mean(0), z2.mean(0)"
      ],
      "metadata": {
        "id": "ELiZmzyxZhjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, indim, classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            jnn.Linear(indim, classes, bias=False),\n",
        "            neuron.IFNode()\n",
        "        )\n",
        "        jF.set_step_mode(self, 'm')\n",
        "    def forward(self, x):\n",
        "        jF.reset_net(self)\n",
        "        y = self.layer(x)\n",
        "        return y.mean(0)"
      ],
      "metadata": {
        "id": "kGhgK9xRkwxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NT_Xent(nn.Module):\n",
        "    def __init__(self, batch_size, tau, device):\n",
        "        super(NT_Xent, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.tau = tau\n",
        "        self.device = device\n",
        "        self.mask = self.make_mask()\n",
        "        self.cosine = nn.CosineSimilarity(dim=2)\n",
        "        self.Xent = nn.CrossEntropyLoss()\n",
        "\n",
        "    def make_mask(self):\n",
        "        mask = torch.eye(2 * self.batch_size, dtype=torch.bool)\n",
        "        for i in range(self.batch_size):\n",
        "            mask[i, i+self.batch_size] = 1\n",
        "            mask[i+self.batch_size, i] = 1\n",
        "        return ~mask\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        z = torch.cat((z1, z2), dim=0).to(self.device)\n",
        "        similarity = self.cosine(z.unsqueeze(1), z.unsqueeze(0)) / self.tau\n",
        "\n",
        "        sim_ij = similarity[range(self.batch_size), range(self.batch_size, 2 * self.batch_size)]\n",
        "        sim_ji = similarity[range(self.batch_size, 2 * self.batch_size), range(self.batch_size)]\n",
        "\n",
        "        positive = torch.cat([sim_ij, sim_ji], dim=0).reshape(2*self.batch_size, 1)\n",
        "        negative = similarity[self.mask].reshape(2*self.batch_size, -1)\n",
        "\n",
        "        labels = torch.zeros(2*self.batch_size, dtype=torch.long).to(self.device)\n",
        "        logits = torch.cat((positive, negative), dim=1)\n",
        "        loss = self.Xent(logits, labels)\n",
        "\n",
        "        return loss / 2"
      ],
      "metadata": {
        "id": "tjhg4XyRPaaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "9ncaaIgoVmnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentation:\n",
        "    def __init__(self):\n",
        "        color_jitter = TF.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "        self.tf = TF.Compose([\n",
        "            TF.RandomResizedCrop(32, (0.36, 1)),\n",
        "            TF.RandomHorizontalFlip(p=0.5),\n",
        "            TF.RandomApply([color_jitter], p=0.8),\n",
        "            TF.RandomGrayscale(p=0.2),\n",
        "            TF.ToImage(),\n",
        "            TF.ToDtype(torch.float32, scale=True)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.tf(x), self.tf(x)"
      ],
      "metadata": {
        "id": "c1QjfoDk1DX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(data='cifar10', split='train', batch_size=128, DA=False):\n",
        "    tf = DataAugmentation() if DA else TF.Compose([TF.ToImage(), TF.ToDtype(torch.float32, scale=True)])\n",
        "    if data == 'cifar10':\n",
        "        match split:\n",
        "            case 'train':\n",
        "                data = datasets.CIFAR10('./data', train=True, transform=tf, download=True)\n",
        "            case 'test':\n",
        "                data = datasets.CIFAR10('./data', train=False, transform=tf, download=True)\n",
        "            case 'all':\n",
        "                train = datasets.CIFAR10('./data', train=True, transform=tf, download=True)\n",
        "                test = datasets.CIFAR10('./data', train=False, transform=tf, download=True)\n",
        "                data = ConcatDataset([train, test])\n",
        "    elif data == 'stl10':\n",
        "        match split:\n",
        "            case 'train':\n",
        "                data = datasets.STL10('./data', split='train', transform=tf, download=True)\n",
        "            case 'test':\n",
        "                data = datasets.STL10('./data', split='test', transform=tf, download=True)\n",
        "            case 'all':\n",
        "                data = datasets.STL10('./data', split='unlabeled', transform=tf, download=True)\n",
        "    else:\n",
        "        print(f'{data} is not supported >_<. cifar10 or stl10 is supported')\n",
        "    loader = DataLoader(data, batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "    return loader"
      ],
      "metadata": {
        "id": "uMIXsB32yVvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_(loader, model, optimizer, scheduler, criterion, device):\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    model.train()\n",
        "    for data, target in tqdm(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        correct += (out.argmax(1) == target).sum().item()\n",
        "    scheduler.step()\n",
        "    return running_loss, correct"
      ],
      "metadata": {
        "id": "QCe3SHrhaGUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_onTPU(loader, model, optimizer, scheduler, criterion, device):\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    model.train()\n",
        "    loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "    for data, target in tqdm(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(xm.xla_device()):\n",
        "            out = model(data)\n",
        "            loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        xm.mark_step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        correct += (out.argmax(1) == target).sum().item()\n",
        "    scheduler.step()\n",
        "    return running_loss, correct"
      ],
      "metadata": {
        "id": "4UpThriKV72x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cl(loader, model, optimizer, scheduler, criterion, device):\n",
        "    running_loss = 0\n",
        "    model.train()\n",
        "    for (x1, x2), _ in tqdm(loader):\n",
        "        x1, x2 = x1.to(device), x2.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        z1, z2 = model(x1, x2)\n",
        "        loss = criterion(z1, z2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    return running_loss"
      ],
      "metadata": {
        "id": "g4lcasnwVpqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cl_onTPU(loader, model, optimizer, scheduler, criterion, device):\n",
        "    running_loss = 0\n",
        "    model.train()\n",
        "    loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "    for (x1, x2), _ in tqdm(loader):\n",
        "        x1, x2 = x1.to(device), x2.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(xm.xla_device()):\n",
        "            z1, z2 = model(x1, x2)\n",
        "            loss = criterion(z1, z2)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        xm.mark_step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    scheduler.step()\n",
        "    return running_loss"
      ],
      "metadata": {
        "id": "i5N_OV5aWJT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_in_cl(loader, encoder, classifier, optimizer, criterion, device):\n",
        "    correct = 0\n",
        "    encoder.eval()\n",
        "    classifier.train()\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            z = encoder(data)\n",
        "        out = classifier(z)\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        correct += (out.argmax(1) == target).sum().item()\n",
        "    return correct"
      ],
      "metadata": {
        "id": "9t9Ow04Q4kZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_in_cl_onTPU(loader, encoder, classifier, optimizer, criterion, device):\n",
        "    correct = 0\n",
        "    encoder.eval()\n",
        "    classifier.train()\n",
        "    loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(xm.xla_device()):\n",
        "            with torch.no_grad():\n",
        "                z = encoder(data)\n",
        "            out = classifier(z)\n",
        "            loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        xm.mark_step()\n",
        "\n",
        "        correct += (out.argmax(1) == target).sum().item()\n",
        "    return correct"
      ],
      "metadata": {
        "id": "zBPxpQPdWotX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(filename, model, optimizer, scheduler):\n",
        "    checkpoint = {\n",
        "        'model_sd': model.state_dict(),\n",
        "        'optimizer_sd': optimizer.state_dict(),\n",
        "        'scheduler_sd': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(checkpoint, f'{filename}.pth')\n",
        "\n",
        "def load_checkpoint(filename, model, optimizer, scheduler):\n",
        "    checkpoint = torch.load(f'{filename}.pth')\n",
        "    model.load_state_dict(checkpoint['model_sd'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_sd'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_sd'])"
      ],
      "metadata": {
        "id": "AQ-911kihgC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "sv6z2_DA_3Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instance\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch_xla.device()\n",
        "#------------------------#\n",
        "loader = get_loader('cifar10', split='all', batch_size=256, DA=True)\n",
        "loader2 = get_loader('cifar10', split='test', batch_size=128, DA=False)\n",
        "N = len(loader2.dataset)\n",
        "#------------------------#\n",
        "encoder = SEW_ResNet(4)\n",
        "projector = Projector()\n",
        "model = SimCLR(encoder, projector).to(device)\n",
        "classifier = Classifier(128, 10).to(device)\n",
        "#------------------------#\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.3)\n",
        "optimizer2 = optim.Adam(classifier.parameters())\n",
        "scheduler1 = lrs.LinearLR(optimizer, start_factor=0.01, total_iters=8)\n",
        "scheduler2 = lrs.CosineAnnealingLR(optimizer, T_max=4, eta_min=1e-1)\n",
        "scheduler = lrs.SequentialLR(optimizer, [scheduler1, scheduler2], milestones=[8])\n",
        "#------------------------#\n",
        "criterion = NT_Xent(256, 0.2, device).to(device)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "wandb.login()\n",
        "run = wandb.init(\n",
        "    project = 'SimCLR SEW 1102',\n",
        "    config = {\n",
        "        'Architecture': 'SEW-ResNet14(dim=128)',\n",
        "        'feature dim': 128,\n",
        "        'embedding dim': 64,\n",
        "        'T': 4,\n",
        "        'optim': 'SGD',\n",
        "        'lr': 0.3,\n",
        "        'sche1': 'Linear(0.01, 8)',\n",
        "        'sche2': 'Cosine(8, 1e-1)',\n",
        "        'sche': 'Seq([8])',\n",
        "        'criterion': 'NT-Xent',\n",
        "        'tau': 0.2,\n",
        "        'Data': 'Cifar10',\n",
        "        'batch': 256,\n",
        "        'else': 'groups=2, down_sample.stride=2, ADD'\n",
        "    }\n",
        ")\n",
        "\n",
        "#train\n",
        "start_epoch = 0\n",
        "epochs = 16\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    loss = train_cl(loader, model, optimizer, scheduler2, criterion, device)\n",
        "    correct = train_in_cl(loader2, model.encoder, classifier, optimizer2, criterion2, device)\n",
        "    wandb.log({'loss': loss, 'acc': correct*100/N})\n",
        "    print(f'Epoch: {epoch} | loss: {loss} | acc: {correct*100/N}%')\n",
        "\n",
        "wandb.finish()\n",
        "save_checkpoint('SimCLR_by_SEW_1107_ADD', model, optimizer, scheduler2)"
      ],
      "metadata": {
        "id": "v-dkMx6Y_7Em",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0af6b4d5-4d4b-454e-c9e7-7272c30dea4e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 27.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtk-0311-h\u001b[0m (\u001b[33mtk-0311-h-hosei-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241107_031120-iaonf0ew</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102/runs/iaonf0ew' target=\"_blank\">balmy-resonance-7</a></strong> to <a href='https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102' target=\"_blank\">https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102/runs/iaonf0ew' target=\"_blank\">https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102/runs/iaonf0ew</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:09<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | loss: 721.524603843689 | acc: 13.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | loss: 716.3026320934296 | acc: 16.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | loss: 698.2833225727081 | acc: 18.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | loss: 673.5129079818726 | acc: 20.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | loss: 627.9863755702972 | acc: 21.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | loss: 570.5778880119324 | acc: 26.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6 | loss: 536.1793036460876 | acc: 27.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7 | loss: 517.0463840961456 | acc: 27.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8 | loss: 496.62525153160095 | acc: 27.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9 | loss: 473.5976436138153 | acc: 30.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | loss: 450.1544636487961 | acc: 31.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11 | loss: 436.1648129224777 | acc: 33.3%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12 | loss: 428.8815670013428 | acc: 33.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13 | loss: 424.7932713031769 | acc: 34.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14 | loss: 424.40572488307953 | acc: 34.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [03:07<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15 | loss: 425.8596637248993 | acc: 33.69%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▂▃▃▄▅▆▆▆▆▇▇████</td></tr><tr><td>loss</td><td>██▇▇▆▄▄▃▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>33.69</td></tr><tr><td>loss</td><td>425.85966</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">balmy-resonance-7</strong> at: <a href='https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102/runs/iaonf0ew' target=\"_blank\">https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102/runs/iaonf0ew</a><br/> View project at: <a href='https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102' target=\"_blank\">https://wandb.ai/tk-0311-h-hosei-university/SimCLR%20SEW%201102</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241107_031120-iaonf0ew/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Result\n",
        "# T4 | 13.2 / 15 GB | batch=512, T=2 | 2m / epoch | 1epで10%, 2epで14%\n",
        "# L4 | 28.9 / 22 GB | same | 1epで14%\n",
        "# TPU | diagでエラー | positiveのdiagをno_grad()の中に だめ -> loaderをparallel_loaderへ(T=1) 1.5min/epochできた\n",
        "# TPU | 上のままTを上げていく T=2でダメ -> mixed precision で T=2 OK, 2min/epoch\n",
        "# 上の記録 | 13.23 -> 16.35 -> 18.44 -> 19.58 -> 19.96 -> 21.24 > 21.53 > 21.8\n",
        "\n",
        "# down_sampleのstride=2, groups=2でやろう. paramsが1200k->500k\n",
        "# T4 GPU | batch=256, T=3(9.8GB) OK(13%) | T=4(12.7GB) できる(13%) | T=5は無理そう\n",
        "# TPU | batch=256, T=1 OK(10.4%) | T=2 OK(12.2%) | T=3 OK(13.2%) | T=4 NO(Exhausted)\n",
        "# ひとまず TPU N=256, T=3, epochs=32, 20%でsaturation\n",
        "# projectorをANNに変えたらResorce Exhausted\n",
        "# L4 GPU | N=256 T=4 epochs=16 | 3min/epoch -> epoch5 17%でsaturation | z1, z2は異常なし\n",
        "\n",
        "# 11/7 z1,z2問題解決\n",
        "# hyper parameterは上と同じ | L4 GPUで3.1min/epoch | RAMは半分 -> まだいけるぞ |\n",
        "# 34%まではうまくいった -> lrが小さすぎて坂を上れなかったか -> T-wiseでlossとる | どうであれもっとepoch増やしてから分析"
      ],
      "metadata": {
        "id": "b-d8ffTYDcoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspection"
      ],
      "metadata": {
        "id": "imIg0hJyzo8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = get_loader('cifar10', split='all', batch_size=128, DA=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU3WJOVr_Bud",
        "outputId": "03f6168d-cb18-4725-b8a6-56035af83eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis\n",
        "device = torch.device('cpu')\n",
        "e = SEW_ResNet(4)\n",
        "m = SimCLR(e, 128, 64)\n",
        "cl = Classifier(128, 10)\n",
        "cr = NT_Xent(128, 0.2, device)\n",
        "#load state dict\n",
        "cp = torch.load('SimCLR_by_SEW_1103_failure.pth', weights_only=True, map_location=device)\n",
        "m.load_state_dict(cp['model_sd'])\n",
        "\n",
        "(d1, d2), t = next(iter(l))\n",
        "z1, z2 = m(d1, d2)\n",
        "print(z1[0])\n",
        "print(z2[0])\n",
        "loss = cr(z1, z2)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "pl0Sd-zZSQMe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5070e7-d991-4502-dece-e5bc6056731e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block2:tensor([[[1., 1., 1., 1.],\n",
            "         [2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2.],\n",
            "         [2., 2., 2., 2.]]], grad_fn=<SliceBackward0>)\n",
            "block4:tensor([[[2., 1., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 0., 1.],\n",
            "         [2., 1., 1., 1.]],\n",
            "\n",
            "        [[2., 1., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 0., 0.],\n",
            "         [2., 2., 1., 1.]],\n",
            "\n",
            "        [[2., 2., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 0., 0.],\n",
            "         [2., 2., 1., 1.]],\n",
            "\n",
            "        [[2., 1., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 0., 0.],\n",
            "         [2., 2., 1., 1.]]], grad_fn=<SliceBackward0>)\n",
            "block6:tensor([[[3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.]],\n",
            "\n",
            "        [[3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.]],\n",
            "\n",
            "        [[3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.]],\n",
            "\n",
            "        [[3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.]]], grad_fn=<SliceBackward0>)\n",
            "hi:tensor([[3.0000, 3.0000, 0.5000, 2.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000,\n",
            "         0.0000, 0.0000, 3.0000, 1.9219, 3.0000, 3.0000, 2.8750, 3.0000, 0.0000,\n",
            "         2.9219, 0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 0.0000, 2.9844, 3.0000,\n",
            "         3.0000, 0.0000, 0.0000, 2.0000, 0.0000, 1.0000, 2.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.9688, 1.3281, 3.0000, 2.5000, 0.8906, 0.0000, 3.0000, 0.0000,\n",
            "         3.0000, 3.0000, 0.0156, 3.0000, 1.9688, 3.0000, 2.9531, 2.5312, 2.8906,\n",
            "         3.0000, 3.0000, 2.3594, 3.0000, 2.9688, 0.0000, 2.9844, 0.0000, 3.0000,\n",
            "         3.0000, 1.0000, 0.9375, 3.0000, 3.0000, 0.0000, 3.0000, 1.2031, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 0.0000, 3.0000, 0.0000, 2.9844, 1.8906, 3.0000,\n",
            "         2.9844, 0.0000, 2.9688, 1.0781, 3.0000, 2.0000, 3.0000, 3.0000, 3.0000,\n",
            "         0.0000, 3.0000, 0.0000, 3.0000, 0.0156, 3.0000, 2.0000, 2.9531, 2.9688,\n",
            "         3.0000, 3.0000, 3.0000, 2.8125, 0.0000, 0.0000, 2.9531, 3.0000, 0.0000,\n",
            "         0.0000, 3.0000, 0.0000, 0.0000, 2.9531, 3.0000, 3.0000, 2.9219, 2.8750,\n",
            "         0.0000, 0.0000, 2.9688, 3.0000, 2.9688, 3.0000, 1.4375, 3.0000, 3.0000,\n",
            "         1.8750, 3.0000],\n",
            "        [3.0000, 3.0000, 0.4844, 2.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000,\n",
            "         0.0000, 0.0000, 3.0000, 1.9531, 3.0000, 3.0000, 2.8750, 3.0000, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 0.0000, 3.0000, 3.0000,\n",
            "         3.0000, 0.0000, 0.0000, 2.0000, 0.0000, 1.0000, 2.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.9688, 1.4688, 3.0000, 2.5625, 0.9688, 0.0000, 3.0000, 0.0000,\n",
            "         3.0000, 3.0000, 0.0156, 3.0000, 1.9688, 3.0000, 3.0000, 2.4844, 2.9531,\n",
            "         3.0000, 3.0000, 2.3906, 2.9375, 3.0000, 0.0000, 2.9844, 0.0000, 3.0000,\n",
            "         3.0000, 1.0000, 0.9688, 3.0000, 3.0000, 0.0000, 3.0000, 1.0469, 0.0000,\n",
            "         2.9844, 0.0000, 0.0000, 0.0000, 3.0000, 0.0000, 2.9531, 1.8125, 3.0000,\n",
            "         3.0000, 0.0000, 3.0000, 1.1250, 3.0000, 2.0156, 3.0000, 3.0000, 2.9844,\n",
            "         0.0000, 3.0000, 0.0000, 3.0000, 0.0469, 3.0000, 2.0000, 2.8906, 2.9844,\n",
            "         3.0000, 3.0000, 3.0000, 2.7656, 0.0000, 0.0000, 2.9375, 2.9688, 0.0000,\n",
            "         0.0000, 3.0000, 0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 2.9844, 2.7500,\n",
            "         0.0000, 0.0000, 3.0000, 3.0000, 2.9531, 3.0000, 1.4688, 3.0000, 3.0000,\n",
            "         1.7969, 2.9375],\n",
            "        [3.0000, 3.0000, 0.4531, 2.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000,\n",
            "         0.0000, 0.0000, 3.0000, 1.9375, 3.0000, 3.0000, 2.9062, 3.0000, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 0.0000, 3.0000, 3.0000,\n",
            "         3.0000, 0.0156, 0.0000, 2.0000, 0.0000, 1.0000, 2.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.9688, 1.4531, 3.0000, 2.5625, 0.9844, 0.0000, 3.0000, 0.0000,\n",
            "         3.0000, 3.0000, 0.0000, 3.0000, 1.9688, 3.0000, 3.0000, 2.5000, 2.9844,\n",
            "         3.0000, 3.0000, 2.4062, 2.9688, 3.0000, 0.0000, 2.9844, 0.0000, 3.0000,\n",
            "         3.0000, 1.0000, 0.9688, 3.0000, 2.9844, 0.0000, 3.0000, 1.0781, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 0.0000, 3.0000, 0.0000, 3.0000, 1.7188, 3.0000,\n",
            "         3.0000, 0.0000, 3.0000, 1.1250, 3.0000, 2.0312, 3.0000, 3.0000, 2.9844,\n",
            "         0.0000, 3.0000, 0.0000, 3.0000, 0.0781, 3.0000, 2.0000, 2.8438, 2.9062,\n",
            "         3.0000, 3.0000, 3.0000, 2.7500, 0.0000, 0.0000, 2.9375, 2.9844, 0.0000,\n",
            "         0.0000, 3.0000, 0.0000, 0.0000, 3.0000, 2.9844, 3.0000, 3.0000, 2.6719,\n",
            "         0.0000, 0.0000, 3.0000, 3.0000, 2.9219, 3.0000, 1.3750, 3.0000, 3.0000,\n",
            "         1.8750, 2.9375],\n",
            "        [3.0000, 3.0000, 0.4688, 2.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000,\n",
            "         0.0000, 0.0000, 3.0000, 1.9531, 3.0000, 3.0000, 2.8906, 3.0000, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 0.0000, 3.0000, 3.0000,\n",
            "         3.0000, 0.0000, 0.0000, 2.0000, 0.0000, 1.0000, 2.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.9688, 1.5156, 3.0000, 2.5938, 1.0000, 0.0000, 3.0000, 0.0000,\n",
            "         3.0000, 3.0000, 0.0000, 3.0000, 1.9688, 3.0000, 3.0000, 2.5000, 2.9844,\n",
            "         3.0000, 3.0000, 2.4219, 2.9375, 3.0000, 0.0000, 3.0000, 0.0000, 3.0000,\n",
            "         3.0000, 1.0000, 0.9844, 3.0000, 2.9844, 0.0000, 3.0000, 1.1406, 0.0000,\n",
            "         3.0000, 0.0000, 0.0156, 0.0000, 3.0000, 0.0000, 2.9844, 1.7969, 3.0000,\n",
            "         3.0000, 0.0000, 3.0000, 1.1094, 3.0000, 2.0000, 3.0000, 3.0000, 2.9844,\n",
            "         0.0000, 3.0000, 0.0000, 3.0000, 0.0781, 3.0000, 2.0000, 2.8594, 2.9375,\n",
            "         3.0000, 3.0000, 3.0000, 2.7500, 0.0000, 0.0156, 2.9375, 2.9844, 0.0000,\n",
            "         0.0000, 3.0000, 0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 3.0000, 2.7500,\n",
            "         0.0000, 0.0000, 3.0000, 3.0000, 2.9219, 3.0000, 1.4688, 3.0000, 3.0000,\n",
            "         1.8125, 2.9375]], grad_fn=<SliceBackward0>)\n",
            " zi:tensor([[ 0.2363, -0.5847,  0.3407, -0.3561,  0.1276, -0.6737, -0.5170, -0.2238,\n",
            "         -0.5798,  0.4633, -0.1560,  1.1178,  0.5937, -0.5736, -0.4888, -0.9678,\n",
            "          0.1009, -0.8594, -0.1085,  0.6639,  0.0105, -0.0584,  0.3451,  0.0315,\n",
            "         -0.4215, -0.5422,  0.0266,  0.3079, -0.4109, -0.2129, -1.4442,  0.0867,\n",
            "         -0.6872, -0.6403,  0.6354,  0.5209, -0.5763,  0.0767,  0.1276, -0.0621,\n",
            "         -0.8714, -0.0607, -0.4384, -0.6590, -0.5753,  0.5564,  0.1063, -0.2942,\n",
            "          0.0140,  0.9530,  0.3700,  0.3804,  0.3403, -0.1313,  0.8070, -0.4103,\n",
            "          0.8686, -0.0440, -0.3631, -0.2911,  0.0282,  0.2573, -0.5398,  0.6144],\n",
            "        [ 0.2363, -0.5847,  0.3407, -0.3561,  0.1276, -0.6737, -0.5170, -0.2238,\n",
            "         -0.5798,  0.4633, -0.1560,  1.1178,  0.5937, -0.5736, -0.4888, -0.9678,\n",
            "          0.1009, -0.8594, -0.1085,  0.6639,  0.0105, -0.0584,  0.3451,  0.0315,\n",
            "         -0.4215, -0.5422,  0.0266,  0.3079, -0.4109, -0.2129, -1.4442,  0.0867,\n",
            "         -0.6872, -0.6403,  0.6354,  0.5209, -0.5763,  0.0767,  0.1276, -0.0621,\n",
            "         -0.8714, -0.0607, -0.4384, -0.6590, -0.5753,  0.5564,  0.1063, -0.2942,\n",
            "          0.0140,  0.9530,  0.3700,  0.3804,  0.3403, -0.1313,  0.8070, -0.4103,\n",
            "          0.8686, -0.0440, -0.3631, -0.2911,  0.0282,  0.2573, -0.5398,  0.6144],\n",
            "        [ 0.2363, -0.5847,  0.3407, -0.3561,  0.1276, -0.6737, -0.5170, -0.2238,\n",
            "         -0.5798,  0.4633, -0.1560,  1.1178,  0.5937, -0.5736, -0.4888, -0.9678,\n",
            "          0.1009, -0.8594, -0.1085,  0.6639,  0.0105, -0.0584,  0.3451,  0.0315,\n",
            "         -0.4215, -0.5422,  0.0266,  0.3079, -0.4109, -0.2129, -1.4442,  0.0867,\n",
            "         -0.6872, -0.6403,  0.6354,  0.5209, -0.5763,  0.0767,  0.1276, -0.0621,\n",
            "         -0.8714, -0.0607, -0.4384, -0.6590, -0.5753,  0.5564,  0.1063, -0.2942,\n",
            "          0.0140,  0.9530,  0.3700,  0.3804,  0.3403, -0.1313,  0.8070, -0.4103,\n",
            "          0.8686, -0.0440, -0.3631, -0.2911,  0.0282,  0.2573, -0.5398,  0.6144],\n",
            "        [ 0.2363, -0.5847,  0.3407, -0.3561,  0.1276, -0.6737, -0.5170, -0.2238,\n",
            "         -0.5798,  0.4633, -0.1560,  1.1178,  0.5937, -0.5736, -0.4888, -0.9678,\n",
            "          0.1009, -0.8594, -0.1085,  0.6639,  0.0105, -0.0584,  0.3451,  0.0315,\n",
            "         -0.4215, -0.5422,  0.0266,  0.3079, -0.4109, -0.2129, -1.4442,  0.0867,\n",
            "         -0.6872, -0.6403,  0.6354,  0.5209, -0.5763,  0.0767,  0.1276, -0.0621,\n",
            "         -0.8714, -0.0607, -0.4384, -0.6590, -0.5753,  0.5564,  0.1063, -0.2942,\n",
            "          0.0140,  0.9530,  0.3700,  0.3804,  0.3403, -0.1313,  0.8070, -0.4103,\n",
            "          0.8686, -0.0440, -0.3631, -0.2911,  0.0282,  0.2573, -0.5398,  0.6144]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "block2:tensor([[[1., 1., 1., 1.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [1., 2., 2., 2.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [1., 2., 2., 2.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [1., 2., 2., 2.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [1., 2., 2., 2.],\n",
            "         [1., 2., 2., 2.]]], grad_fn=<SliceBackward0>)\n",
            "block4:tensor([[[3., 2., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 0., 1.],\n",
            "         [2., 1., 1., 1.]],\n",
            "\n",
            "        [[3., 2., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 1., 1.]],\n",
            "\n",
            "        [[3., 2., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 0., 1.],\n",
            "         [2., 1., 0., 1.]],\n",
            "\n",
            "        [[3., 2., 1., 1.],\n",
            "         [2., 1., 1., 1.],\n",
            "         [2., 1., 0., 1.],\n",
            "         [2., 1., 0., 1.]]], grad_fn=<SliceBackward0>)\n",
            "block6:tensor([[[3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.]],\n",
            "\n",
            "        [[3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.]],\n",
            "\n",
            "        [[3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.]],\n",
            "\n",
            "        [[3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.],\n",
            "         [3., 3., 3., 3.]]], grad_fn=<SliceBackward0>)\n",
            "hj:tensor([[3.0000, 3.0000, 0.5938, 2.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000,\n",
            "         0.0000, 0.0000, 3.0000, 1.9219, 3.0000, 3.0000, 2.7500, 3.0000, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 3.0000, 2.9844, 3.0000, 0.0000, 3.0000, 3.0000,\n",
            "         3.0000, 0.0000, 0.0000, 2.0000, 0.0000, 1.0000, 1.9844, 0.0000, 0.0000,\n",
            "         0.0000, 0.8438, 1.2031, 2.9844, 2.5156, 1.0000, 0.0000, 2.9844, 0.0000,\n",
            "         3.0000, 3.0000, 0.0156, 3.0000, 1.9688, 3.0000, 3.0000, 2.5000, 3.0000,\n",
            "         3.0000, 3.0000, 2.0469, 3.0000, 3.0000, 0.0000, 2.9844, 0.0000, 3.0000,\n",
            "         3.0000, 1.0000, 0.8438, 3.0000, 3.0000, 0.0000, 3.0000, 1.0938, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 0.0000, 3.0000, 0.0000, 3.0000, 1.9531, 3.0000,\n",
            "         3.0000, 0.0000, 3.0000, 1.1562, 3.0000, 1.9844, 3.0000, 3.0000, 2.9844,\n",
            "         0.0156, 3.0000, 0.0000, 3.0000, 0.0000, 2.9844, 2.0000, 2.9688, 2.9375,\n",
            "         3.0000, 3.0000, 3.0000, 2.7812, 0.0000, 0.0156, 2.9844, 2.9844, 0.0000,\n",
            "         0.0000, 3.0000, 0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 3.0000, 2.9375,\n",
            "         0.0000, 0.0000, 3.0000, 3.0000, 2.9688, 3.0000, 1.4688, 3.0000, 3.0000,\n",
            "         1.5156, 2.9531],\n",
            "        [3.0000, 3.0000, 0.5469, 2.0000, 3.0000, 3.0000, 3.0000, 3.0000, 3.0000,\n",
            "         0.0000, 0.0000, 3.0000, 2.0000, 3.0000, 3.0000, 2.7656, 3.0000, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 3.0000, 2.9844, 3.0000, 0.0000, 3.0000, 3.0000,\n",
            "         3.0000, 0.0000, 0.0000, 2.0000, 0.0000, 1.0000, 1.9844, 0.0000, 0.0000,\n",
            "         0.0000, 0.8750, 1.3125, 2.9844, 2.5156, 1.0000, 0.0000, 3.0000, 0.0000,\n",
            "         3.0000, 3.0000, 0.0156, 3.0000, 1.9688, 3.0000, 3.0000, 2.4688, 3.0000,\n",
            "         3.0000, 3.0000, 2.0312, 2.9531, 3.0000, 0.0000, 2.9688, 0.0000, 3.0000,\n",
            "         3.0000, 1.0000, 0.8594, 3.0000, 3.0000, 0.0000, 3.0000, 1.0625, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 0.0000, 3.0000, 0.0000, 3.0000, 1.9688, 3.0000,\n",
            "         3.0000, 0.0000, 3.0000, 1.1562, 3.0000, 1.9844, 3.0000, 3.0000, 2.9844,\n",
            "         0.0156, 3.0000, 0.0000, 3.0000, 0.0156, 2.9844, 2.0000, 2.9688, 2.9531,\n",
            "         3.0000, 3.0000, 3.0000, 2.8125, 0.0000, 0.0469, 2.9688, 2.9375, 0.0000,\n",
            "         0.0000, 3.0000, 0.0000, 0.0000, 3.0000, 2.9844, 2.9844, 3.0000, 2.9375,\n",
            "         0.0000, 0.0000, 2.9844, 3.0000, 2.9531, 3.0000, 1.4688, 3.0000, 3.0000,\n",
            "         1.4375, 2.9375],\n",
            "        [3.0000, 3.0000, 0.5625, 2.0000, 3.0000, 3.0000, 3.0000, 2.9688, 3.0000,\n",
            "         0.0000, 0.0000, 3.0000, 1.9688, 3.0000, 3.0000, 2.6562, 3.0000, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 3.0000, 2.9688, 3.0000, 0.0000, 3.0000, 2.9688,\n",
            "         3.0000, 0.0156, 0.0000, 1.9844, 0.0000, 1.0000, 1.9688, 0.0000, 0.0000,\n",
            "         0.0000, 0.8281, 1.1719, 2.9688, 2.5156, 1.0000, 0.0000, 2.9844, 0.0000,\n",
            "         3.0000, 3.0000, 0.0156, 3.0000, 1.9688, 3.0000, 3.0000, 2.4844, 3.0000,\n",
            "         3.0000, 3.0000, 2.0156, 2.9688, 3.0000, 0.0000, 2.9531, 0.0000, 3.0000,\n",
            "         2.9844, 1.0000, 0.8125, 3.0000, 3.0000, 0.0000, 3.0000, 1.0156, 0.0000,\n",
            "         2.9688, 0.0000, 0.0000, 0.0000, 3.0000, 0.0000, 3.0000, 1.9531, 3.0000,\n",
            "         3.0000, 0.0000, 3.0000, 1.1250, 3.0000, 1.9844, 3.0000, 3.0000, 2.9844,\n",
            "         0.0156, 2.9844, 0.0000, 3.0000, 0.0000, 2.9844, 2.0000, 2.9688, 2.9375,\n",
            "         3.0000, 3.0000, 3.0000, 2.8281, 0.0000, 0.0469, 2.9844, 2.9219, 0.0000,\n",
            "         0.0000, 3.0000, 0.0000, 0.0000, 3.0000, 3.0000, 3.0000, 3.0000, 2.9219,\n",
            "         0.0000, 0.0000, 2.8594, 3.0000, 2.9531, 2.9844, 1.5156, 3.0000, 3.0000,\n",
            "         1.4688, 2.9375],\n",
            "        [3.0000, 3.0000, 0.6250, 1.9688, 3.0000, 3.0000, 3.0000, 2.9688, 2.9844,\n",
            "         0.0000, 0.0000, 3.0000, 2.0312, 2.9844, 3.0000, 2.7031, 3.0000, 0.0000,\n",
            "         3.0000, 0.0000, 0.0000, 3.0000, 2.9688, 3.0000, 0.0000, 3.0000, 2.9688,\n",
            "         3.0000, 0.0156, 0.0000, 1.9844, 0.0000, 1.0000, 1.9688, 0.0000, 0.0000,\n",
            "         0.0000, 0.8281, 1.2812, 2.9688, 2.5000, 1.0000, 0.0000, 2.9844, 0.0000,\n",
            "         3.0000, 3.0000, 0.0000, 3.0000, 1.9375, 2.9844, 3.0000, 2.4062, 3.0000,\n",
            "         3.0000, 2.9844, 2.0000, 2.9688, 3.0000, 0.0000, 2.9375, 0.0000, 3.0000,\n",
            "         2.9844, 1.0000, 0.7500, 2.9531, 2.9688, 0.0000, 3.0000, 1.0156, 0.0000,\n",
            "         2.9219, 0.0000, 0.0000, 0.0000, 2.9844, 0.0000, 2.9844, 1.9375, 3.0000,\n",
            "         3.0000, 0.0000, 3.0000, 1.1719, 2.9844, 1.9688, 3.0000, 3.0000, 2.9844,\n",
            "         0.0000, 2.9531, 0.0000, 3.0000, 0.0000, 2.9688, 2.0000, 2.9375, 2.8906,\n",
            "         3.0000, 3.0000, 3.0000, 2.7812, 0.0000, 0.0625, 2.9531, 2.9219, 0.0000,\n",
            "         0.0000, 3.0000, 0.0000, 0.0000, 3.0000, 3.0000, 2.9844, 2.9844, 2.8906,\n",
            "         0.0000, 0.0000, 2.9062, 3.0000, 2.9375, 2.9688, 1.4688, 3.0000, 3.0000,\n",
            "         1.4688, 2.9219]], grad_fn=<SliceBackward0>)\n",
            " zj:tensor([[ 0.2363, -0.5847,  0.3407, -0.3561,  0.1276, -0.6737, -0.5170, -0.2238,\n",
            "         -0.5798,  0.4633, -0.1560,  1.1178,  0.5937, -0.5736, -0.4888, -0.9678,\n",
            "          0.1009, -0.8594, -0.1085,  0.6639,  0.0105, -0.0584,  0.3451,  0.0315,\n",
            "         -0.4215, -0.5422,  0.0266,  0.3079, -0.4109, -0.2129, -1.4442,  0.0867,\n",
            "         -0.6872, -0.6403,  0.6354,  0.5209, -0.5763,  0.0767,  0.1276, -0.0621,\n",
            "         -0.8714, -0.0607, -0.4384, -0.6590, -0.5753,  0.5564,  0.1063, -0.2942,\n",
            "          0.0140,  0.9530,  0.3700,  0.3804,  0.3403, -0.1313,  0.8070, -0.4103,\n",
            "          0.8686, -0.0440, -0.3631, -0.2911,  0.0282,  0.2573, -0.5398,  0.6144],\n",
            "        [ 0.2491, -0.5372,  0.3542, -0.4121,  0.1830, -0.7315, -0.4660, -0.2107,\n",
            "         -0.5021,  0.5318, -0.1766,  1.0878,  0.5610, -0.6569, -0.4219, -0.9640,\n",
            "          0.1593, -0.7855, -0.1615,  0.6490, -0.0110,  0.0164,  0.3432, -0.0418,\n",
            "         -0.3573, -0.4865,  0.0205,  0.3931, -0.4315, -0.2440, -1.4685,  0.1729,\n",
            "         -0.6671, -0.5956,  0.7102,  0.4733, -0.6053,  0.1526,  0.0952, -0.0631,\n",
            "         -0.7845, -0.0735, -0.3605, -0.7108, -0.5087,  0.5370,  0.0938, -0.2243,\n",
            "          0.0266,  0.9715,  0.3143,  0.3285,  0.3147, -0.0704,  0.7920, -0.4701,\n",
            "          0.8426, -0.0902, -0.2827, -0.3241, -0.0109,  0.3192, -0.4818,  0.5736],\n",
            "        [ 0.2491, -0.5372,  0.3542, -0.4121,  0.1830, -0.7315, -0.4660, -0.2107,\n",
            "         -0.5021,  0.5318, -0.1766,  1.0878,  0.5610, -0.6569, -0.4219, -0.9640,\n",
            "          0.1593, -0.7855, -0.1615,  0.6490, -0.0110,  0.0164,  0.3432, -0.0418,\n",
            "         -0.3573, -0.4865,  0.0205,  0.3931, -0.4315, -0.2440, -1.4685,  0.1729,\n",
            "         -0.6671, -0.5956,  0.7102,  0.4733, -0.6053,  0.1526,  0.0952, -0.0631,\n",
            "         -0.7845, -0.0735, -0.3605, -0.7108, -0.5087,  0.5370,  0.0938, -0.2243,\n",
            "          0.0266,  0.9715,  0.3143,  0.3285,  0.3147, -0.0704,  0.7920, -0.4701,\n",
            "          0.8426, -0.0902, -0.2827, -0.3241, -0.0109,  0.3192, -0.4818,  0.5736],\n",
            "        [ 0.2491, -0.5372,  0.3542, -0.4121,  0.1830, -0.7315, -0.4660, -0.2107,\n",
            "         -0.5021,  0.5318, -0.1766,  1.0878,  0.5610, -0.6569, -0.4219, -0.9640,\n",
            "          0.1593, -0.7855, -0.1615,  0.6490, -0.0110,  0.0164,  0.3432, -0.0418,\n",
            "         -0.3573, -0.4865,  0.0205,  0.3931, -0.4315, -0.2440, -1.4685,  0.1729,\n",
            "         -0.6671, -0.5956,  0.7102,  0.4733, -0.6053,  0.1526,  0.0952, -0.0631,\n",
            "         -0.7845, -0.0735, -0.3605, -0.7108, -0.5087,  0.5370,  0.0938, -0.2243,\n",
            "          0.0266,  0.9715,  0.3143,  0.3285,  0.3147, -0.0704,  0.7920, -0.4701,\n",
            "          0.8426, -0.0902, -0.2827, -0.3241, -0.0109,  0.3192, -0.4818,  0.5736]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "tensor([ 0.2363, -0.5847,  0.3407, -0.3561,  0.1276, -0.6737, -0.5170, -0.2238,\n",
            "        -0.5798,  0.4633, -0.1560,  1.1178,  0.5937, -0.5736, -0.4888, -0.9678,\n",
            "         0.1009, -0.8594, -0.1085,  0.6639,  0.0105, -0.0584,  0.3451,  0.0315,\n",
            "        -0.4215, -0.5422,  0.0266,  0.3079, -0.4109, -0.2129, -1.4442,  0.0867,\n",
            "        -0.6872, -0.6403,  0.6354,  0.5209, -0.5763,  0.0767,  0.1276, -0.0621,\n",
            "        -0.8714, -0.0607, -0.4384, -0.6590, -0.5753,  0.5564,  0.1063, -0.2942,\n",
            "         0.0140,  0.9530,  0.3700,  0.3804,  0.3403, -0.1313,  0.8070, -0.4103,\n",
            "         0.8686, -0.0440, -0.3631, -0.2911,  0.0282,  0.2573, -0.5398,  0.6144],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([ 2.4589e-01, -5.4905e-01,  3.5079e-01, -3.9807e-01,  1.6915e-01,\n",
            "        -7.1709e-01, -4.7872e-01, -2.1398e-01, -5.2154e-01,  5.1471e-01,\n",
            "        -1.7145e-01,  1.0953e+00,  5.6915e-01, -6.3604e-01, -4.3864e-01,\n",
            "        -9.6497e-01,  1.4473e-01, -8.0397e-01, -1.4823e-01,  6.5270e-01,\n",
            "        -5.6413e-03, -2.3224e-03,  3.4369e-01, -2.3451e-02, -3.7332e-01,\n",
            "        -5.0041e-01,  2.2051e-02,  3.7176e-01, -4.2634e-01, -2.3626e-01,\n",
            "        -1.4624e+00,  1.5137e-01, -6.7214e-01, -6.0677e-01,  6.9151e-01,\n",
            "         4.8521e-01, -5.9804e-01,  1.3363e-01,  1.0331e-01, -6.2838e-02,\n",
            "        -8.0624e-01, -7.0305e-02, -3.7999e-01, -6.9786e-01, -5.2533e-01,\n",
            "         5.4183e-01,  9.6921e-02, -2.4175e-01,  2.3473e-02,  9.6688e-01,\n",
            "         3.2819e-01,  3.4148e-01,  3.2107e-01, -8.5654e-02,  7.9573e-01,\n",
            "        -4.5515e-01,  8.4906e-01, -7.8672e-02, -3.0284e-01, -3.1585e-01,\n",
            "        -1.1284e-03,  3.0370e-01, -4.9628e-01,  5.8384e-01],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(2.7707, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    }
  ]
}